{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Imports for the HParams plugin\n",
    "from tensorboard.plugins.hparams import api_pb2\n",
    "from tensorboard.plugins.hparams import summary as hparams_summary\n",
    "from google.protobuf import struct_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptrons\n",
    "\n",
    "## Experiment setup\n",
    "\n",
    "This experiment is to observe the effect of hypereparameters (listed below) on the accuracy of a shallow neural network.\n",
    "* Learning rate\n",
    "* Number of units on the hidden layer\n",
    "* Mini-batch size\n",
    "* Drop out rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [0.001, 0.01, 0.1]\n",
    "num_units_list = [16, 32]\n",
    "mini_batch_size_list = [32, 64]\n",
    "dropout_rate_list = [0., 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inform the HParams dashboard the hyperparameters and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_summary(learning_rate_list, num_units_list, mini_batch_size_list, dropout_rate_list):\n",
    "    learning_rate_list_val = struct_pb2.ListValue()\n",
    "    learning_rate_list_val.extend(learning_rate_list)\n",
    "    \n",
    "    num_units_list_val = struct_pb2.ListValue()\n",
    "    num_units_list_val.extend(num_units_list)\n",
    "    \n",
    "    mini_batch_size_list_val = struct_pb2.ListValue()\n",
    "    mini_batch_size_list_val.extend(mini_batch_size_list)\n",
    "    \n",
    "    dropout_rate_list_val = struct_pb2.ListValue()\n",
    "    dropout_rate_list_val.extend(dropout_rate_list)\n",
    "    \n",
    "    return hparams_summary.experiment_pb(\n",
    "      # The hyperparameters being changed\n",
    "      hparam_infos=[\n",
    "          api_pb2.HParamInfo(name='learning_rate',\n",
    "                             display_name='Learning Rate',\n",
    "                             type=api_pb2.DATA_TYPE_FLOAT64,\n",
    "                             domain_discrete=learning_rate_list_val),\n",
    "          api_pb2.HParamInfo(name='num_units',\n",
    "                             display_name='Number of units',\n",
    "                             type=api_pb2.DATA_TYPE_FLOAT64,\n",
    "                             domain_discrete=num_units_list_val),\n",
    "          api_pb2.HParamInfo(name='mini_batch_size',\n",
    "                             display_name='Mini Batch Size',\n",
    "                             type=api_pb2.DATA_TYPE_FLOAT64,\n",
    "                             domain_discrete=mini_batch_size_list_val),\n",
    "          api_pb2.HParamInfo(name='dropout_rate',\n",
    "                             display_name='Dropout rate',\n",
    "                             type=api_pb2.DATA_TYPE_FLOAT64,\n",
    "                             domain_discrete=dropout_rate_list_val),\n",
    "      ],\n",
    "      # The metrics being tracked\n",
    "      metric_infos=[\n",
    "          api_pb2.MetricInfo(\n",
    "              name=api_pb2.MetricName(\n",
    "                  tag='accuracy'),\n",
    "              display_name='Accuracy'),\n",
    "      ]\n",
    "    )\n",
    "\n",
    "\n",
    "exp_summary = create_experiment_summary(learning_rate_list, num_units_list, mini_batch_size_list, dropout_rate_list)\n",
    "root_logdir_writer = tf.summary.create_file_writer(\"logs/hparam_tuning\")\n",
    "with root_logdir_writer.as_default():\n",
    "    tf.summary.import_event(tf.compat.v1.Event(summary=exp_summary).SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt TensorFlow run to log hyperparameters and metric\n",
    "The difference compared to regular training routine is the hyperparameters are no longer hardcoded. Instead, they are provided as `hparam` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(hparams):\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(hparams['num_units'], activation=tf.nn.relu),\n",
    "                tf.keras.layers.Dropout(hparams['dropout_rate']),\n",
    "                tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "                ])\n",
    "    \n",
    "    optim = tf.keras.optimizers.Adam(lr=hparams['learning_rate'])\n",
    "    \n",
    "    model.compile(optimizer=optim,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, \n",
    "              y_train,\n",
    "              batch_size=hparams['mini_batch_size'],\n",
    "              epochs=1) # Run with 1 epoch to speed things up for demo purposes\n",
    "    \n",
    "    _, accuracy = model.evaluate(x_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each run, log an hparams summary with the hyperparameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    writer = tf.summary.create_file_writer(run_dir)\n",
    "    summary_start = hparams_summary.session_start_pb(hparams=hparams)\n",
    "\n",
    "    with writer.as_default():\n",
    "        accuracy = train_mlp(hparams)\n",
    "        summary_end = hparams_summary.session_end_pb(api_pb2.STATUS_SUCCESS)\n",
    "\n",
    "        tf.summary.scalar('accuracy', accuracy, step=1, description=\"The accuracy\")\n",
    "        tf.summary.import_event(tf.compat.v1.Event(summary=summary_start).SerializeToString())\n",
    "        tf.summary.import_event(tf.compat.v1.Event(summary=summary_end).SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start running & log the accuaracy with different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running training session 1\n",
      "{'learning_rate': 0.001, 'num_units': 16, 'mini_batch_size': 32, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.4428 - accuracy: 0.8737\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.2789 - accuracy: 0.9188\n",
      "--- Running training session 2\n",
      "{'learning_rate': 0.001, 'num_units': 16, 'mini_batch_size': 32, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 1.0461 - accuracy: 0.6345\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.4029 - accuracy: 0.8974\n",
      "--- Running training session 3\n",
      "{'learning_rate': 0.001, 'num_units': 16, 'mini_batch_size': 64, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.5282 - accuracy: 0.8552\n",
      "10000/10000 [==============================] - 1s 54us/sample - loss: 0.2901 - accuracy: 0.9193\n",
      "--- Running training session 4\n",
      "{'learning_rate': 0.001, 'num_units': 16, 'mini_batch_size': 64, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 1.1663 - accuracy: 0.5841\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.4708 - accuracy: 0.8896\n",
      "--- Running training session 5\n",
      "{'learning_rate': 0.001, 'num_units': 32, 'mini_batch_size': 32, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.3631 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.2175 - accuracy: 0.9360\n",
      "--- Running training session 6\n",
      "{'learning_rate': 0.001, 'num_units': 32, 'mini_batch_size': 32, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.7492 - accuracy: 0.7642\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2906 - accuracy: 0.9213\n",
      "--- Running training session 7\n",
      "{'learning_rate': 0.001, 'num_units': 32, 'mini_batch_size': 64, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.4249 - accuracy: 0.8840\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.2640 - accuracy: 0.9250\n",
      "--- Running training session 8\n",
      "{'learning_rate': 0.001, 'num_units': 32, 'mini_batch_size': 64, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.7830 - accuracy: 0.7484\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.3031 - accuracy: 0.9167\n",
      "--- Running training session 9\n",
      "{'learning_rate': 0.01, 'num_units': 16, 'mini_batch_size': 32, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.3699 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.2808 - accuracy: 0.9213\n",
      "--- Running training session 10\n",
      "{'learning_rate': 0.01, 'num_units': 16, 'mini_batch_size': 32, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1.2991 - accuracy: 0.5083\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.5773 - accuracy: 0.8585\n",
      "--- Running training session 11\n",
      "{'learning_rate': 0.01, 'num_units': 16, 'mini_batch_size': 64, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.3312 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.2425 - accuracy: 0.9307\n",
      "--- Running training session 12\n",
      "{'learning_rate': 0.01, 'num_units': 16, 'mini_batch_size': 64, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.9815 - accuracy: 0.6490\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.3750 - accuracy: 0.8956\n",
      "--- Running training session 13\n",
      "{'learning_rate': 0.01, 'num_units': 32, 'mini_batch_size': 32, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.2668 - accuracy: 0.9207\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.2151 - accuracy: 0.9286\n",
      "--- Running training session 14\n",
      "{'learning_rate': 0.01, 'num_units': 32, 'mini_batch_size': 32, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.8783 - accuracy: 0.6931\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.3344 - accuracy: 0.9134\n",
      "--- Running training session 15\n",
      "{'learning_rate': 0.01, 'num_units': 32, 'mini_batch_size': 64, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2669 - accuracy: 0.9203\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.1745 - accuracy: 0.9454\n",
      "--- Running training session 16\n",
      "{'learning_rate': 0.01, 'num_units': 32, 'mini_batch_size': 64, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.7389 - accuracy: 0.7440\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.2869 - accuracy: 0.9169\n",
      "--- Running training session 17\n",
      "{'learning_rate': 0.1, 'num_units': 16, 'mini_batch_size': 32, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 1.5202 - accuracy: 0.4355\n",
      "10000/10000 [==============================] - 1s 62us/sample - loss: 1.5027 - accuracy: 0.4286\n",
      "--- Running training session 18\n",
      "{'learning_rate': 0.1, 'num_units': 16, 'mini_batch_size': 32, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 2.3091 - accuracy: 0.1091\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 2.2812 - accuracy: 0.1292\n",
      "--- Running training session 19\n",
      "{'learning_rate': 0.1, 'num_units': 16, 'mini_batch_size': 64, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1.3781 - accuracy: 0.4930\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 1.2325 - accuracy: 0.5428\n",
      "--- Running training session 20\n",
      "{'learning_rate': 0.1, 'num_units': 16, 'mini_batch_size': 64, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 2.1312 - accuracy: 0.1941\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 2.1200 - accuracy: 0.1758\n",
      "--- Running training session 21\n",
      "{'learning_rate': 0.1, 'num_units': 32, 'mini_batch_size': 32, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 1.4110 - accuracy: 0.4912\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 1.8121 - accuracy: 0.3710\n",
      "--- Running training session 22\n",
      "{'learning_rate': 0.1, 'num_units': 32, 'mini_batch_size': 32, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 2.2760 - accuracy: 0.1372\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 2.3023 - accuracy: 0.1033\n",
      "--- Running training session 23\n",
      "{'learning_rate': 0.1, 'num_units': 32, 'mini_batch_size': 64, 'dropout_rate': 0.0}\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 1.2236 - accuracy: 0.5341\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 1.2374 - accuracy: 0.5593\n",
      "--- Running training session 24\n",
      "{'learning_rate': 0.1, 'num_units': 32, 'mini_batch_size': 64, 'dropout_rate': 0.5}\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 2.0864 - accuracy: 0.2213\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 1.9077 - accuracy: 0.2684\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "    for num_units in num_units_list:\n",
    "        for mini_batch_size in mini_batch_size_list:\n",
    "            for dropout_rate in dropout_rate_list:\n",
    "                hparams = {'learning_rate':learning_rate, \n",
    "                           'num_units': num_units, \n",
    "                           'mini_batch_size': mini_batch_size,\n",
    "                           'dropout_rate': dropout_rate\n",
    "                          }\n",
    "                print('--- Running training session %d' % (session_num + 1))\n",
    "                print(hparams)\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                run(\"logs/hparam_tuning/\" + run_name, hparams)\n",
    "                session_num += 1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
